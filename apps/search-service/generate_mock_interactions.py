import json
import os
import random
import uuid
import pandas as pd
from faker import Faker
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy
from datetime import datetime, timedelta
from collections import Counter, defaultdict

# ---------------------------------------------------------
# 1. C·∫§U H√åNH
# ---------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
JSON_DIR = os.path.join(BASE_DIR, "jsons")

# Input files
HOTEL_FILE = os.path.join(JSON_DIR, "__homeStay.json")
USER_FILE = os.path.join(JSON_DIR, "__users.json")

# Output files
OUTPUT_INTERACTIONS_FILE = os.path.join(JSON_DIR, "__interactions.json")
OUTPUT_REVIEWS_FILE = os.path.join(JSON_DIR, "__reviews.json")
OUTPUT_METRICS_FILE = os.path.join(JSON_DIR, "__metrics.json")
OUTPUT_DAILY_STATS_FILE = os.path.join(JSON_DIR, "__daily_stats.json") # File m·ªõi

fake = Faker()

POSITIVE_COMMENTS = [
    "Ph√≤ng ·ªëc r·∫•t s·∫°ch s·∫Ω, view ƒë·∫πp tuy·ªát v·ªùi.",
    "Nh√¢n vi√™n nhi·ªát t√¨nh, ƒë·ªãa ƒëi·ªÉm thu·∫≠n l·ª£i.",
    "Tr·∫£i nghi·ªám tuy·ªát v·ªùi, ch·∫Øc ch·∫Øn s·∫Ω quay l·∫°i.",
    "Gi√° c·∫£ h·ª£p l√Ω so v·ªõi ch·∫•t l∆∞·ª£ng ph·ª•c v·ª•.",
    "Kh√¥ng gian y√™n tƒ©nh, th√≠ch h·ª£p ngh·ªâ d∆∞·ª°ng.",
    "B·ªÉ b∆°i v√¥ c·ª±c r·∫•t ƒë·∫πp, ƒë·ªì ƒÉn ngon.",
    # B·ªï sung th√™m 5 b√¨nh lu·∫≠n t√≠ch c·ª±c
    "Check-in nhanh ch√≥ng, ph√≤ng ƒë∆∞·ª£c upgrade mi·ªÖn ph√≠, b·∫•t ng·ªù d·ªÖ ch·ªãu!",
    "Ban c√¥ng r·ªông, ng·∫Øm ho√†ng h√¥n c·ª±c chill. R·∫•t ƒë√°ng ti·ªÅn!",
    "ƒê·ªám √™m, ga g·ªëi th∆°m tho, ng·ªß ngon su·ªët ƒë√™m.",
    "D·ªãch v·ª• d·ªçn ph√≤ng chuy√™n nghi·ªáp, thay khƒÉn m·ªói ng√†y.",
    "G·∫ßn bi·ªÉn, ƒëi b·ªô 2 ph√∫t l√† t·ªõi. View t·ª´ ph√≤ng si√™u ∆∞ng!"
]

NEGATIVE_COMMENTS = [
    "Ph√≤ng h∆°i c≈©, c√°ch √¢m kh√¥ng t·ªët.",
    "Nh√¢n vi√™n l·ªÖ t√¢n th√°i ƒë·ªô ch∆∞a t·ªët.",
    "V·ªã tr√≠ h∆°i xa trung t√¢m, ƒëi l·∫°i b·∫•t ti·ªán.",
    "Wifi y·∫øu, kh√¥ng l√†m vi·ªác ƒë∆∞·ª£c.",
    "B·ªØa s√°ng √≠t m√≥n, kh√¥ng h·ª£p kh·∫©u v·ªã.",
    "V·ªá sinh ch∆∞a s·∫°ch, c√≤n b·ª•i b·∫©n.",
    # B·ªï sung th√™m 5 b√¨nh lu·∫≠n ti√™u c·ª±c
    "M√πi ·∫©m m·ªëc trong ph√≤ng, m·ªü c·ª≠a c·∫£ ng√†y v·∫´n kh√¥ng h·∫øt.",
    "G·ªçi l·ªÖ t√¢n 3 l·∫ßn m·ªõi c√≥ ng∆∞·ªùi ph·∫£n h·ªìi, qu√° ch·∫≠m!",
    "H√¨nh ·∫£nh tr√™n web ƒë·∫πp h∆°n th·ª±c t·∫ø nhi·ªÅu, c·∫£m gi√°c b·ªã l·ª´a.",
    "ƒêi·ªÅu h√≤a k√™u to, ·∫£nh h∆∞·ªüng gi·∫•c ng·ªß ban ƒë√™m.",
    "Kh√¥ng c√≥ ch·ªó ƒë·ªÉ xe an to√†n, ph·∫£i g·ª≠i ngo√†i ƒë∆∞·ªùng."
]

# ---------------------------------------------------------
# 2. H√ÄM H·ªñ TR·ª¢
# ---------------------------------------------------------
def load_json(filepath):
    if not os.path.exists(filepath):
        return None
    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)

# H√†m sinh Tuning Params gi·∫£ l·∫≠p (cho bi·ªÉu ƒë·ªì SVD)
def generate_tuning_params(base_rmse):
    # Gi·∫£ l·∫≠p: K tƒÉng th√¨ RMSE gi·∫£m d·∫ßn
    data = []
    for k in [10, 20, 30, 40, 50, 60]:
        noise = random.uniform(-0.02, 0.02)
        # RMSE gi·∫£m d·∫ßn theo K
        metric = base_rmse + (60 - k) * 0.002 + noise 
        data.append({"param": k, "metric": round(metric, 4)})
    return data

# ---------------------------------------------------------
# 3. LOGIC CH√çNH
# ---------------------------------------------------------
def generate_data():
    hotels = load_json(HOTEL_FILE)
    if not hotels:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file hotel.")
        return

    # 1. Load User
    users_data = load_json(USER_FILE)
    users = []
    if users_data:
        for u in users_data:
            users.append({"id": u["id"]})
    else:
        # Fallback t·∫°o user ·∫£o
        for i in range(1, 51):
            users.append({"id": f"user_ai_{i}"})

    interactions = []
    reviews = []
    
    # D√πng dictionary ƒë·ªÉ c·ªông d·ªìn DailyStat ngay khi sinh Interaction
    daily_agg = defaultdict(lambda: {
        "totalRevenue": 0, "totalBookings": 0, "totalCancels": 0,
        "totalViews": 0, "totalClickBook": 0, "totalLikes": 0, "totalSearch": 0
    })

    print("ü§ñ ƒêang sinh d·ªØ li·ªáu m√¥ ph·ªèng Session & DailyStats...")
    
    current_time = datetime.now()

    # Sinh 2000 Sessions (M·ªói session l√† 1 chu·ªói h√†nh ƒë·ªông c·ªßa 1 user)
    for _ in range(2000): 
        user = random.choice(users)
        
        # T·∫°o Session ID
        session_id = f"sess_{uuid.uuid4().hex[:12]}"
        
        # Ch·ªçn ng·∫´u nhi√™n ng√†y trong 6 th√°ng qua
        days_back = random.randint(0, 180)
        base_time = current_time - timedelta(days=days_back)
        date_key = base_time.strftime("%Y-%m-%d") # Key cho DailyStat

        # M·ªói session user xem t·ª´ 1-5 kh√°ch s·∫°n
        num_viewed = random.randint(1, 5)
        
        for _ in range(num_viewed):
            hotel = random.choice(hotels)
            hotel_id = hotel['id']
            price = hotel.get('price', 1000000)

            # Th·ªùi gian h√†nh ƒë·ªông di·ªÖn ra sau base_time v√†i gi√¢y/ph√∫t
            offset_seconds = random.randint(10, 3600)
            timestamp_obj = base_time + timedelta(seconds=offset_seconds)
            timestamp = timestamp_obj.isoformat()

            # 1. VIEW (Lu√¥n c√≥)
            interactions.append({
                "userId": user["id"], "hotelId": hotel_id, "sessionId": session_id,
                "type": "VIEW", "rating": None,
                "timestamp": timestamp, "metadata": {"duration": random.randint(30, 300)}
            })
            daily_agg[date_key]["totalViews"] += 1

            # 2. LIKE (Random 20%)
            if random.random() < 0.2:
                interactions.append({
                    "userId": user["id"], "hotelId": hotel_id, "sessionId": session_id,
                    "type": "LIKE", "rating": None,
                    "timestamp": timestamp, "metadata": {}
                })
                daily_agg[date_key]["totalLikes"] += 1

            # 3. CLICK BOOK (High Intent - Random 15%)
            if random.random() < 0.15:
                interactions.append({
                    "userId": user["id"], "hotelId": hotel_id, "sessionId": session_id,
                    "type": "CLICK_BOOK_NOW", "rating": None,
                    "timestamp": timestamp, "metadata": {}
                })
                daily_agg[date_key]["totalClickBook"] += 1

                # 4. BOOK CONFIRMED (Conversion - 80% c·ªßa Click Book)
                if random.random() < 0.8:
                    interactions.append({
                        "userId": user["id"], "hotelId": hotel_id, "sessionId": session_id,
                        "type": "BOOK", "rating": None,
                        "timestamp": timestamp,
                        "metadata": {"amount": price}
                    })
                    daily_agg[date_key]["totalBookings"] += 1
                    daily_agg[date_key]["totalRevenue"] += price

                    # 5. CANCEL (Random 5% sau khi Book)
                    if random.random() < 0.05:
                        # T·∫°o l·ªánh h·ªßy sau ƒë√≥ v√†i ng√†y
                        cancel_time = (timestamp_obj + timedelta(days=random.randint(1, 3))).isoformat()
                        interactions.append({
                            "userId": user["id"], "hotelId": hotel_id, "sessionId": session_id,
                            "type": "CANCEL", "rating": None,
                            "timestamp": cancel_time, "metadata": {}
                        })
                        # C·∫≠p nh·∫≠t th·ªëng k√™ h·ªßy (L∆∞u √Ω: Th∆∞·ªùng tr·ª´ doanh thu ·ªü ng√†y h·ªßy ho·∫∑c ng√†y ƒë·∫∑t t√πy logic, ·ªü ƒë√¢y tr·ª´ ng√†y ƒë·∫∑t cho ƒë∆°n gi·∫£n dashboard)
                        daily_agg[date_key]["totalCancels"] += 1
                        daily_agg[date_key]["totalRevenue"] -= price # Ho√†n ti·ªÅn
                    
                    else:
                        # 6. REVIEW (Ch·ªâ review n·∫øu ƒë√£ book v√† kh√¥ng h·ªßy)
                        # T·∫°o rating gi·∫£
                        rating = random.choices([5, 4, 3, 2, 1], weights=[50, 30, 10, 5, 5])[0]
                        comment = random.choice(POSITIVE_COMMENTS if rating >=4 else NEGATIVE_COMMENTS)
                        
                        # L∆∞u v√†o b·∫£ng Review
                        reviews.append({
                            "userId": user["id"], "hotelId": hotel_id, "rating": rating,
                            "comment": comment, "sentiment": "POSITIVE" if rating >=4 else "NEGATIVE",
                            "createdAt": (timestamp_obj + timedelta(days=2)).isoformat()
                        })
                        # L∆∞u Interaction Rating (ƒë·ªÉ sync data)
                        interactions.append({
                            "userId": user["id"], "hotelId": hotel_id, "sessionId": session_id,
                            "type": "RATING", "rating": rating,
                            "timestamp": (timestamp_obj + timedelta(days=2)).isoformat(),
                            "metadata": {}
                        })

    # --- SAVE FILES ---
    with open(OUTPUT_INTERACTIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(interactions, f, ensure_ascii=False, indent=2)
    
    with open(OUTPUT_REVIEWS_FILE, "w", encoding="utf-8") as f:
        json.dump(reviews, f, ensure_ascii=False, indent=2)

    # X·ª≠ l√Ω Daily Stats t·ª´ dictionary ra list
    daily_stats_list = []
    for date_str, stats in daily_agg.items():
        daily_stats_list.append({
            "date": f"{date_str}T00:00:00.000Z", # Format chu·∫©n ISO cho Prisma DateTime
            **stats
        })
    
    with open(OUTPUT_DAILY_STATS_FILE, "w", encoding="utf-8") as f:
        json.dump(daily_stats_list, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ ƒê√£ t·∫°o: {len(daily_stats_list)} ng√†y th·ªëng k√™ (DailyStat).")

    # -------------------------------------------------
    # C. TRAIN AI MODEL & METRICS (C·∫≠p nh·∫≠t SystemMetric m·ªõi)
    # -------------------------------------------------
    print("üß† ƒêang gi·∫£ l·∫≠p System Metrics & Tuning Params...")
    
    # Gi·∫£ l·∫≠p metrics trong 30 ng√†y g·∫ßn ƒë√¢y
    historical_metrics = []
    
    for i in range(29, -1, -1):
        date_str = (datetime.now() - timedelta(days=i)).isoformat()
        
        # Gi·∫£ l·∫≠p c·∫£i thi·ªán d·∫ßn theo th·ªùi gian
        base_rmse = 0.95 - (i * 0.005) # C√†ng v·ªÅ hi·ªán t·∫°i RMSE c√†ng th·∫•p (t·ªët)
        base_rmse = max(0.80, base_rmse + random.uniform(-0.02, 0.02))

        metric_entry = {
            "rmse": round(base_rmse, 4),
            "precisionAt5": round(70 + random.uniform(-5, 5), 2),
            "recallAt5": round(60 + random.uniform(-5, 5), 2),
            "algorithm": "SVD",
            "datasetSize": 1000 + (30-i)*50,
            "executionTimeMs": random.randint(100, 500), # NEW FIELD
            "createdAt": date_str,
            # NEW FIELD: JSON Tuning Params (Ch·ªâ th√™m v√†o b·∫£n ghi m·ªõi nh·∫•t ho·∫∑c t·∫•t c·∫£ t√πy b·∫°n)
            "tuningParams": generate_tuning_params(base_rmse) if i == 0 else None, 
            "trainingHistory": None # SVD kh√¥ng c√≥ epoch history, ƒë·ªÉ null
        }
        historical_metrics.append(metric_entry)

    with open(OUTPUT_METRICS_FILE, "w", encoding="utf-8") as f:
        json.dump(historical_metrics, f, indent=2)
            
    print(f"üìä ƒê√£ t·∫°o {len(historical_metrics)} b·∫£n ghi Metrics.")
    print("üéâ HO√ÄN T·∫§T! Copy file JSON v√†o th∆∞ m·ª•c seed.")

if __name__ == "__main__":
    os.makedirs(JSON_DIR, exist_ok=True)
    generate_data()