import json
import torch
from sentence_transformers import SentenceTransformer
from PIL import Image
import requests
from io import BytesIO
import os

# ---------------------------------------------------------
# 1. KH·ªûI T·∫†O MODELS AI
# ---------------------------------------------------------
print("‚è≥ ƒêang t·∫£i models AI...")

# Model 1: CLIP (X·ª≠ l√Ω ·∫£nh) - Output: 512 dims
# D√πng ƒë·ªÉ: T√¨m kh√°ch s·∫°n b·∫±ng h√¨nh ·∫£nh t∆∞∆°ng ƒë·ªìng
img_model = SentenceTransformer("clip-ViT-B-32")

# Model 2: Multilingual Text (X·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát) - Output: 512 dims
# D√πng ƒë·ªÉ: RAG, t√¨m ki·∫øm ng·ªØ nghƒ©a (vd: "t√¨m ch·ªó ·ªü cho gia ƒë√¨nh c√≥ b·∫øp")
# distiluse-base-multilingual-cased-v1 h·ªó tr·ª£ 50+ ng√¥n ng·ªØ g·ªìm Ti·∫øng Vi·ªát
text_model = SentenceTransformer("distiluse-base-multilingual-cased-v1")

print("‚úÖ Models ƒë√£ s·∫µn s√†ng!")

# ---------------------------------------------------------
# 2. H√ÄM X·ª¨ L√ù ·∫¢NH (GI·ªÆ NGUY√äN LOGIC C≈®)
# ---------------------------------------------------------
def get_image_vector(path_or_url):
    try:
        if not path_or_url: return None
        
        # Tr∆∞·ªùng h·ª£p 1: URL Online
        if path_or_url.startswith(("http://", "https://")):
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            # Timeout ng·∫Øn ƒë·ªÉ tr√°nh treo l√¢u n·∫øu link ch·∫øt
            response = requests.get(path_or_url, headers=headers, timeout=5)
            if response.status_code != 200: return None
            img = Image.open(BytesIO(response.content)).convert("RGB")
        
        # Tr∆∞·ªùng h·ª£p 2: Local Path
        else:
            current_dir = os.getcwd() 
            project_root = os.path.abspath(os.path.join(current_dir, "../../")) 
            clean_path = path_or_url.lstrip("/")
            
            # Logic t√¨m file th√¥ng minh
            possible_paths = [
                os.path.join(project_root, clean_path),
                os.path.join(project_root, "apps/client/public", clean_path),
                os.path.join(current_dir, clean_path)
            ]
            
            full_local_path = None
            for p in possible_paths:
                if os.path.exists(p):
                    full_local_path = p
                    break
            
            if not full_local_path:
                # print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ·∫£nh local: {clean_path}") # B·ªõt spam log
                return None
                
            img = Image.open(full_local_path).convert("RGB")

        # Encode ·∫£nh b·∫±ng CLIP
        vector = img_model.encode(img).tolist()
        return vector

    except Exception as e:
        # print(f"‚ùå L·ªói x·ª≠ l√Ω ·∫£nh {path_or_url}: {e}")
        return None

# ---------------------------------------------------------
# 3. H√ÄM X·ª¨ L√ù TEXT (M·ªöI)
# ---------------------------------------------------------
def get_text_embedding(text):
    """
    Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n (policies, description) th√†nh vector 512 chi·ªÅu
    """
    if not text or len(text.strip()) == 0:
        return None
    try:
        # Encode vƒÉn b·∫£n b·∫±ng Multilingual Model
        vector = text_model.encode(text).tolist()
        return vector
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω text: {e}")
        return None

# ---------------------------------------------------------
# 4. MAIN PROGRAM
# ---------------------------------------------------------
def main():
    input_file = "jsons/__homeStay.json"
    output_file = "jsons/__hotel_vectors.json"

    if not os.path.exists(input_file):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file input: {input_file}")
        return

    with open(input_file, "r", encoding="utf-8") as f:
        stays = json.load(f)

    processed_data = []
    total = len(stays)
    print(f"üöÄ B·∫Øt ƒë·∫ßu t·∫°o vector cho {total} kh√°ch s·∫°n...")

    for index, item in enumerate(stays):
        print(f"[{index+1}/{total}] üõ†Ô∏è  Processing: {item.get('title', 'Unknown')}")
        
        # 1. T·∫°o Image Vector (cho featuredImage)
        img_vec = get_image_vector(item.get("featuredImage"))
        
        # 2. T·∫°o Policies Vector (Context cho RAG)
        # M·∫πo: K·∫øt h·ª£p nhi·ªÅu tr∆∞·ªùng text l·∫°i ƒë·ªÉ AI hi·ªÉu ng·ªØ c·∫£nh t·ªët h∆°n
        # Ch√∫ng ta n·ªëi: Title + Full Description + Policies + Tags
        context_text = f"""
        T√™n: {item.get('title')}
        M√¥ t·∫£: {item.get('fullDescription')}
        Ch√≠nh s√°ch: {item.get('policies')}
        Tags: {', '.join(item.get('tags', []))}
        Ph√π h·ª£p cho: {', '.join(item.get('suitableFor', []))}
        """.strip()
        
        text_vec = get_text_embedding(context_text)

        # Ch·ªâ l∆∞u n·∫øu c√≥ d·ªØ li·ªáu
        if img_vec or text_vec:
            processed_data.append({
                "id": item["id"],
                "imageVector": img_vec,      # Map v√†o schema: imageVector
                "policiesVector": text_vec   # Map v√†o schema: policiesVector
            })

    # L∆∞u k·∫øt qu·∫£
    os.makedirs("jsons", exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(processed_data, f) # Kh√¥ng c·∫ßn indent ƒë·ªÉ file nh·∫π h∆°n
    
    print(f"\n‚úÖ HO√ÄN TH√ÄNH! ƒê√£ xu·∫•t {len(processed_data)} vectors ra file: {output_file}")

if __name__ == "__main__":
    main()